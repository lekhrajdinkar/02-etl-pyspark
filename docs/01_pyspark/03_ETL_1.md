## ETL - things to do:
### Data **Ingestion**
  - multiple sources : CSV, JSON, Parquet, databases, or APIs
  - structured, semi-structured, or unstructured

### Data **Transformation** (with/without schema validation)
  - type casting
  - Filtering
  - aggregating
  - complex business logic for derived columns and calculated new col
  - standardize formatting : datatime, etc

### Data **Cleansing**
  - Handling missing, null --> handled.
  - inconsistent schema (corrupt data) --> not handled, causing failures
  - handle duplicates --> handling it

### Data **Enrichment**
  - Joining datasets from multiple sources to create enriched views.
  - Adding metadata or contextual information to datasets. (creating digest file.)

### Data **Loading**
  - Writing data in batches or streaming mode in RDB
  - to warehouses : s3

###  **Performance Optimization** ⬅️ focus here for pyspark sol
  - `Batch Processing` for large datasets
  - `parallel processing`
  - `partitioning`
  - `concurrent DB connections`
  - `caching`

### **fault tolerance** ⬅️ focus here for pyspark sol
  - `retry mechanism` for transient failures :
    - database connectivity
    - file system unavailability

### **masking, encryption**

### **more:** `aws+k8s` takes care
- Job Scheduling and Orchestration
- Monitoring and Logging
- Scalability