## Small workload etl
- My organization uses spark for small workload etl. 
- load data from `csv file` into `database`.
- **no cluster setup**.
  - running  on single ecs container / eks pod (job) with ASG
  ```
  ECS task definition:
  {
    "cpu": "2048",
    "memory": "4096"
  }
  prevent underutilized containers for lightweight jobs*
  ```
  - No Overhead of Cluster Management
  - Easily scalable by allocating more resources (CPU/memory) to the container, if needed.
  - Maximum size of csv file is 1GB - **don't require distributed processing**.
  - No recovery from container failures :(
  - `future plan` : **moving to Kubernetes**, for running Spark in a distributed manner :point_left:
- **trigger**:
  - s3 file drop >> event >> lambda >> rest api (etl) 
  - internal db-based dependency mgt.
    - job-1 completed --> triggers job-2
- **logging/monitor**
  - CloudWatch Log
  - Enable `Spark's web UI` for monitoring job execution.
  - Airflow : only poc done.
- **Tuning**:
```
  - SparkSession.builder.master("local[*]").appName("ETLJob").getOrCreate()
    - use local[*] : to utilize all available CPU cores
    - parallelism within each container.
  
  - --conf "spark.executor.extraJavaOptions=-XX:+UseG1GC" : gc
  
  - built-in CSV reader** : 
  data = spark.read.csv("file.csv", header=True, inferSchema=True)
  
  - Use DataFrame.write with batch size
  data.write.jdbc(url="jdbc:mysql://db", table="table_name", mode="append", properties={"user": "root", "password": "password", "batchsize": 1000})
  
  - Split large CSV files into smaller chunks for parallel processing
  - adjust : spark.default.parallelism
  
  -  database supports connection pooling (e.g., through HikariCP)

```


