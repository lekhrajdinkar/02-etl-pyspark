{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"00_panda/","title":"Panda","text":""},{"location":"00_panda/#1-intro","title":"1. Intro","text":"<ul> <li>small to medium-sized datasets that fit into the memory of a single machine</li> <li>simple and intuitive API for beginners and data scientists.</li> <li>Faster : due to <code>in-memory operations</code>.</li> <li><code>python-lib</code> only. not polyglot like pySpark</li> <li>no fault tolerance. :(</li> </ul>"},{"location":"01_setup_locally/","title":"references","text":"<ul> <li>https://chatgpt.com/c/674f855d-41b4-800d-a76f-39b5e7bff18c - setup Dockerfile</li> <li>https://chatgpt.com/c/67548d97-6268-800d-a8af-b8ccf576e5f0 - Theory-1</li> <li>https://chatgpt.com/c/6754cff4-b63c-800d-9e2f-caba7735697e - project</li> <li>https://chatgpt.com/c/67554038-a4d4-800d-a0aa-d51f621943a5 - Spark UI</li> <li>https://chatgpt.com/c/67578727-cefc-800d-9b0d-20eee5402296 - trandformation and action on df</li> <li>readme.md </li> </ul>"},{"location":"01_setup_locally/#install","title":"install","text":"<ul> <li>Python:</li> <li>3.9.6 </li> <li>3.12</li> <li>Java 17: </li> <li>not higher.</li> <li>else, java.lang.UnsupportedOperationException: getSubject is supported only if a security manager is allowed.     <pre><code># check java installtion dir\n/usr/libexec/java_home -V\n\nMatching Java Virtual Machines (2):\n    23.0.1 (x86_64) \"Oracle Corporation\" - \"OpenJDK 23.0.1\" /Users/lekhrajdinkar/Library/Java/JavaVirtualMachines/openjdk-23.0.1/Contents/Home\n    17.0.10 (x86_64) \"Oracle Corporation\" - \"Java SE 17.0.10\" /Library/Java/JavaVirtualMachines/jdk-17.jdk/Contents/Home\n\nexport JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk-17.jdk/Contents/Home\nexport PATH=$JAVA_HOME/bin:$PATH\n</code></pre></li> <li>Spark: </li> <li>https://www.apache.org/dyn/closer.lua/spark/spark-3.5.3/spark-3.5.3-bin-hadoop3.tgz     <pre><code>export SPARK_HOME=/Users/lekhrajdinkar/Documents/spark-3.5.3-bin-hadoop3\nexport PATH=$SPARK_HOME/bin:$PATH\n</code></pre></li> <li>hadoop:</li> <li> <p>https://github.com/steveloughran/winutils ( for windows only )       ```     === MAC : spark-3.5.3-bin-hadoop3  spark and hadoop , same ===</p> <p>export HADOOP_HOME=Users/lekhrajdinkar/Documents/spark-3.5.3-bin-hadoop3 export PATH=$HADOOP_HOME/bin:$PATH export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop</p> <p>=== windows : winutils ===</p> <p>set HADOOP_HOME=C:\\Users\\Manisha\\Documents\\winutils-master\\hadoop-3.0.0 set PATH=%HADOOP_HOME%\\bin;%PATH% set HADOOP_CONF_DIR=$HADOOP_HOME\\etc\\hadoop ```</p> </li> </ul>"},{"location":"01_setup_locally/#start-spark-session","title":"start spark session","text":""},{"location":"01_setup_locally/#spark-shell","title":"spark-shell","text":"<pre><code>spark-shell\n\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n24/12/07 13:01:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nSpark context Web UI available at http://192.168.0.255:4040\nSpark context available as 'sc' (master = local[*], app id = local-1733605263311).\nSpark session available as 'spark'.\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.5.3\n      /_/\n\nUsing Scala version 2.12.18 (Java HotSpot(TM) 64-Bit Server VM, Java 17.0.10)\nType in expressions to have them evaluated.\nType :help for more information.\n\nscala&gt; \n</code></pre>"},{"location":"01_setup_locally/#pyspark","title":"pyspark","text":"<ul> <li>pip install pyspark</li> <li>pyspark </li> <li>pyspark --master local[] ``` pyspark --master local[]</li> </ul> <p>Python 3.9.6 (default, Feb  3 2024, 15:58:28)  [Clang 15.0.0 (clang-1500.3.9.4)] on darwin Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. Setting default log level to \"WARN\". To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel). 24/12/07 13:03:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable 24/12/07 13:03:16 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041. Welcome to       ____              __      / /  ___ _____/ /__      \\/ _ \\/ _ `/ __/  '/    /__ / .__/_,// //_   version 3.5.3       //</p> <p>Using Python version 3.9.6 (default, Feb  3 2024 15:58:28) Spark context Web UI available at http://192.168.0.255:4041 Spark context available as 'sc' (master = local[*], app id = local-1733605396364). SparkSession available as 'spark'.</p> <p>```</p>"},{"location":"02_architecture/","title":"02_architecture","text":""},{"location":"02_architecture/#-httpschatgptcomc6756481b-c08c-800d-9b78-2e024a506c16-parquet-and-orc-format","title":"- https://chatgpt.com/c/6756481b-c08c-800d-9b78-2e024a506c16 - parquet and orc format","text":""},{"location":"02_architecture/#a-sparkintro","title":"A. Spark::<code>Intro</code>","text":"<ul> <li><code>polyglot</code> (Scala, Python [PySpark], Java, R, and SQL) + CLI</li> <li>wide libraries for a variety of tasks.</li> <li>100 times faster than MapReduce for processing large amounts of data.</li> <li> <p><code>open-source</code> engine for data processing on computer clusters</p> </li> <li> <p>Designed for processing large-scale datasets on <code>distributed machines</code> (parallel)</p> </li> <li>unstructured, semi-structured, and structured data</li> <li>Powerful Caching</li> <li>in-memory processing / real-time computation</li> </ul>"},{"location":"02_architecture/#use-case","title":"use case:","text":"<ul> <li>for big data analytics,</li> <li>machine learning on large datasets.</li> <li>ETL (Extract, Transform, Load) pipelines</li> </ul>"},{"location":"02_architecture/#deployment-model-for-distributed-architecture","title":"<code>deployment model</code> for distributed architecture:","text":"<ul> <li>cluster setup can be done with :</li> <li>Mesos</li> <li>YARN (Hadoop)</li> <li>Kubernetes</li> <li>Standalone: Spark\u2019s own cluster manager (Local mode)<ul> <li>this is what used in local project</li> <li>ccgg etl project</li> </ul> </li> </ul>"},{"location":"02_architecture/#fault-tolerance","title":"fault tolerance","text":"<ul> <li>Automatic recovery mechanisms </li> <li>If a partition is lost, Spark can recompute it using the lineage graph</li> </ul>"},{"location":"02_architecture/#b-sparkcore-component","title":"B. Spark::<code>core-Component</code>","text":"<pre><code>+---------------------+      +-------------------------+\n|  Driver Program     | ---&gt; |   Cluster Manager       |\n+---------------------+      +-------------------------+\n           |\n           V\n+---------------------+      +-------------------------+\n|   Worker Node 1     | ---&gt; |   Executor 1 (Tasks)    |\n+---------------------+      +-------------------------+\n           |\n           V\n+---------------------+      +-------------------------+\n|   Worker Node 2     | ---&gt; |   Executor 2 (Tasks)    |\n+---------------------+      +-------------------------+\n</code></pre>"},{"location":"02_architecture/#1-rdd","title":"1. RDD","text":"<ul> <li>Resilient Distributed Dataset</li> <li>immutable</li> <li>partitioned collections of data</li> </ul>"},{"location":"02_architecture/#11-dataframes-df","title":"1.1 dataframes / df","text":"<ul> <li>Higher-level abstractions built on top of <code>RDD</code></li> <li><code>Structured</code> data with a schema</li> <li>dataset - typed version of df.</li> <li>Spark-SQL </li> <li>module for processing structured data using SQL-like queries.</li> <li>like joins, aggregations, and filtering</li> </ul>"},{"location":"02_architecture/#2-cluster-manager-node","title":"2. cluster-manager node","text":""},{"location":"02_architecture/#21-driver-program","title":"2.1. driver program","text":"<ul> <li>runs cluster-manager node</li> <li>entry point of the PySpark-app-1</li> <li>duties:</li> <li>Initiates <code>SparkContext</code><ul> <li>Acts as the gateway to the Spark cluster for PySpark-app-1</li> <li>object to manage lifecycle of PySpark-app-1</li> </ul> </li> <li>Creates <code>RDDs/DataFrames</code> <ul> <li>Defines the transformations </li> <li>actions for data processing. </li> </ul> </li> <li><code>Schedules Tasks</code><ul> <li>Breaks the job into stages and task for execution on worker nodes. </li> </ul> </li> <li> <p><code>Collects Results</code> </p> <ul> <li>Aggregates results from executors and returns them to the client.</li> </ul> </li> <li> <p>resource management across the cluster</p> </li> <li>assigns resources (CPU, memory) to the <code>driver</code> and <code>executor nodes</code></li> </ul>"},{"location":"02_architecture/#3-worker-nodes","title":"3. worker node/s","text":""},{"location":"02_architecture/#31-job-dag","title":"3.1. Job / DAG","text":"<ul> <li>highest-level unit of work in Spark.</li> <li>think f --&gt; entire execution path from the input-data to the final-result</li> <li>When an action is invoked on df, Spark computes the full Directed Acyclic Graph (<code>DAG</code>)</li> <li>DAG is divided into stages, based on transformations.</li> <li>stages are distributed to worker nodes as tasks. </li> <li>job-1 </li> <li>stage-1 (task-1)<ul> <li>task-1.1 ( on partition-1)</li> <li>task-1.2 ( on partition-2)</li> <li>... <pre><code># 1. This action triggers a Spark job\n\ndf = spark.read.csv(\"data.csv\")\ndf = df.filter(df[\"age\"] &gt; 30)\ndf = df.groupBy(\"city\").count()\ndf.show()\n\n# 2. DAG\n[Input Data] --&gt; [Filter] --&gt; [Shuffle] --&gt; [GroupBy] --&gt; [Count]\n\n# 3. split in task/stages\nStage 1: Input Data --&gt; Filter\nStage 2: Shuffle --&gt; GroupBy --&gt; Count\n</code></pre></li> </ul> </li> </ul>"},{"location":"02_architecture/#32-task","title":"3.2. Task","text":"<ul> <li>Tasks are <code>units of work</code> created by dividing the data into partitions.</li> <li>Tasks run in parallel across <code>executors</code>.</li> <li>operations:</li> <li>transformations<ul> <li>map, </li> <li>filter</li> <li>...</li> </ul> </li> <li>actions<ul> <li>count</li> <li>collect</li> <li>...</li> </ul> </li> </ul>"},{"location":"02_architecture/#32-executors","title":"3.2. Executors","text":"<ul> <li>task executor.</li> <li><code>distributed processes</code> that run on worker nodes.</li> <li>Executes the <code>tasks</code> assigned by the driver (on partitions of the data)</li> <li>caching/persistence</li> <li>Storing data in memory or disk for intermediate computation</li> <li>Reporting task status</li> <li>send results back to the driver</li> </ul>"},{"location":"02_architecture/#c-sparkecosystem","title":"C. Spark::<code>Ecosystem</code>","text":""},{"location":"02_architecture/#core-api","title":"Core API","text":""},{"location":"02_architecture/#spark-sql","title":"Spark SQL","text":""},{"location":"02_architecture/#spark-streaming","title":"Spark streaming","text":""},{"location":"02_architecture/#spark-mlib","title":"Spark Mlib","text":""},{"location":"02_architecture/#spark-graphx","title":"spark GraphX","text":""},{"location":"03_ETL_1/","title":"03_ETL_1","text":""},{"location":"03_ETL_1/#etl-things-to-do","title":"ETL - things to do:","text":""},{"location":"03_ETL_1/#data-ingestion","title":"Data Ingestion","text":"<ul> <li>multiple sources : CSV, JSON, Parquet, databases, or APIs</li> <li>structured, semi-structured, or unstructured</li> </ul>"},{"location":"03_ETL_1/#data-transformation-withwithout-schema-validation","title":"Data Transformation (with/without schema validation)","text":"<ul> <li>type casting</li> <li>Filtering</li> <li>aggregating</li> <li>complex business logic for derived columns and calculated new col</li> <li>standardize formatting : datatime, etc</li> </ul>"},{"location":"03_ETL_1/#data-cleansing","title":"Data Cleansing","text":"<ul> <li>Handling missing, null --&gt; handled.</li> <li>inconsistent schema (corrupt data) --&gt; not handled, causing failures</li> <li>handle duplicates --&gt; handling it</li> </ul>"},{"location":"03_ETL_1/#data-enrichment","title":"Data Enrichment","text":"<ul> <li>Joining datasets from multiple sources to create enriched views.</li> <li>Adding metadata or contextual information to datasets. (creating digest file.)</li> </ul>"},{"location":"03_ETL_1/#data-loading","title":"Data Loading","text":"<ul> <li>Writing data in batches or streaming mode in RDB</li> <li>to warehouses : s3</li> </ul>"},{"location":"03_ETL_1/#performance-optimization-focus-here-for-pyspark-sol","title":"Performance Optimization  focus here for pyspark sol","text":"<ul> <li><code>Batch Processing</code> for large datasets</li> <li><code>parallel processing</code></li> <li><code>partitioning</code></li> <li><code>concurrent DB connections</code></li> <li><code>caching</code></li> </ul>"},{"location":"03_ETL_1/#fault-tolerance-focus-here-for-pyspark-sol","title":"fault tolerance  focus here for pyspark sol","text":"<ul> <li><code>retry mechanism</code> for transient failures :<ul> <li>database connectivity</li> <li>file system unavailability</li> </ul> </li> </ul>"},{"location":"03_ETL_1/#masking-encryption","title":"masking, encryption","text":""},{"location":"03_ETL_1/#more-awsk8s-takes-care","title":"more: <code>aws+k8s</code> takes care","text":"<ul> <li>Job Scheduling and Orchestration</li> <li>Monitoring and Logging</li> <li>Scalability</li> </ul>"},{"location":"03_ETL_2/","title":"03_ETL_2","text":""},{"location":"03_ETL_2/#small-workload-etl","title":"Small workload etl","text":"<ul> <li>My organization uses spark for small workload etl. </li> <li>load data from <code>csv file</code> into <code>database</code>.</li> <li>no cluster setup.</li> <li>running  on single ecs container / eks pod (job) with ASG   <pre><code>ECS task definition:\n{\n  \"cpu\": \"2048\",\n  \"memory\": \"4096\"\n}\nprevent underutilized containers for lightweight jobs*\n</code></pre></li> <li>No Overhead of Cluster Management</li> <li>Easily scalable by allocating more resources (CPU/memory) to the container, if needed.</li> <li>Maximum size of csv file is 1GB - don't require distributed processing.</li> <li>No recovery from container failures :(</li> <li><code>future plan</code> : moving to Kubernetes, for running Spark in a distributed manner </li> <li>trigger:</li> <li>s3 file drop &gt;&gt; event &gt;&gt; lambda &gt;&gt; rest api (etl) </li> <li>internal db-based dependency mgt.<ul> <li>job-1 completed --&gt; triggers job-2</li> </ul> </li> <li>logging/monitor</li> <li>CloudWatch Log</li> <li>Enable <code>Spark's web UI</code> for monitoring job execution.</li> <li>Airflow : only poc done.</li> <li>Tuning: <pre><code>  - SparkSession.builder.master(\"local[*]\").appName(\"ETLJob\").getOrCreate()\n    - use local[*] : to utilize all available CPU cores\n    - parallelism within each container.\n\n  - --conf \"spark.executor.extraJavaOptions=-XX:+UseG1GC\" : gc\n\n  - built-in CSV reader** : \n  data = spark.read.csv(\"file.csv\", header=True, inferSchema=True)\n\n  - Use DataFrame.write with batch size\n  data.write.jdbc(url=\"jdbc:mysql://db\", table=\"table_name\", mode=\"append\", properties={\"user\": \"root\", \"password\": \"password\", \"batchsize\": 1000})\n\n  - Split large CSV files into smaller chunks for parallel processing\n  - adjust : spark.default.parallelism\n\n  -  database supports connection pooling (e.g., through HikariCP)\n</code></pre></li> </ul>"},{"location":"04_file_format/","title":"Columnar storage formats (for big data processing)","text":"<ul> <li>parquet_demo.py</li> <li>columnar : all the values of a single column are stored together</li> <li>Advantages:<ul> <li>Efficiency for Analytical Queries</li> <li>Better Compression: Values in a column are often similar, making compression more efficient.</li> <li>Predicate Pushdown: Enables faster filtering. <pre><code># Column\nRow 1: id=1, name=\"Alice\", salary=1000\nRow 2: id=2, name=\"Bob\", salary=1500\n\n# Columnar\nColumn \"id\": 1, 2, 3\nColumn \"name\": \"Alice\", \"Bob\", \"Charlie\"\nColumn \"salary\": 1000, 1500, 2000\n</code></pre></li> </ul> </li> </ul>"},{"location":"04_file_format/#a-parquet","title":"A Parquet","text":"<ul> <li>support Schema Evolution.</li> <li>Columnar storage format optimized for analytical queries.</li> <li>Optimized for reading specific columns in wide tables.</li> <li>efficient compression and encoding.</li> <li>Compression: Supports multiple compression codecs (e.g., Snappy, GZIP, ZSTD).</li> <li>use case:</li> <li>If your workload involves accessing a subset of columns from a large dataset. <pre><code># == install  \nbrew install parquet-tools  # MacOS\napt-get install parquet-tools  # Ubuntu\n\n    # inspect\n    parquet-tools schema output/sample.parquet\n    parquet-tools head output/sample.parquet\n</code></pre></li> </ul>"},{"location":"04_file_format/#b-orc","title":"B ORC","text":"<ul> <li>designed by the Apache Hive team for <code>Hadoop</code>.</li> <li>Focuses on high compression ratios and high-speed I/O.</li> <li>advanced compression techniques (e.g., ZLIB, Snappy, LZO).</li> <li>for better query performance (min/max values, row counts, etc.).</li> <li>Built-in indexes</li> <li>Enables filtering at the storage layer for <code>faster queries</code>.</li> <li>Block Splitting: <ul> <li>Data split into stripes, </li> <li>improving parallel processing.</li> </ul> </li> <li>use case :</li> <li>If working in a Hadoop/Hive ecosystem or need advanced indexing and compression for large datasets</li> </ul>"},{"location":"04_file_format/#c-avro","title":"C Avro","text":"<ul> <li>Row-based</li> <li>better for streaming</li> </ul>"},{"location":"100_pyspark_01_start/","title":"interview topic","text":""},{"location":"100_pyspark_01_start/#0-create","title":"0 Create","text":"<ul> <li>data = [1,2,3,4,5]</li> <li>rdd = spark.parallelize(data)</li> <li>spark.createDataFrame(rdd, schema)</li> </ul>"},{"location":"100_pyspark_01_start/#1-select-delete","title":"1 select / delete","text":"<ul> <li>df.select(\"col-1\") - chooses existing columns</li> </ul>"},{"location":"100_pyspark_01_start/#2-transformation-update","title":"2 transformation / update","text":"<ul> <li>lazy operations that create a new RDD / df</li> <li>don't execute until an action is called</li> <li>Each transformation returns a new DataFrame</li> <li>chain transformations for better readability</li> <li>map()</li> <li>Applies a function/lambda to each row of the DataFrame</li> <li>Row-wise transformations,  One row at a time.</li> <li>mapPartition(): Applies a function/lambda to each partition (instead of each row) <pre><code>def lambda1(iterator):\n  for row in iterator:\n    yield (row.number * 10,)\ndf.rdd.map(lambda x: x * 2)\ndf.rdd.map(lambda1)\n\ndf.filter(df.age &gt; 30).show()\ndf.where(df.age &gt; 30).show()\n\ndf.withColumn(\"age_plus_10\", col(\"age\") + 10).show() # adds/modifies a column\n\ndf.withColumnRenamed(\"age\", \"years\").show()\n\n# ==== handle NULL  ===\ndf.na.drop().show()                       # drop rows with any null\ndf.na.drop(subset=[\"age\"]).show()         # drop rows with null in age\n\ndf.na.fill(0).show()                      # fill all nulls with 0\ndf.na.fill({\"age\": 0, \"name\": \"Unknown\"}).show()  # column-specific\n\n# ==== handle duplicate  ===\ndf.dropDuplicates() # expensive\ndf.dropDuplicates(subset=['key_column'])  # Specify only necessary columns\n\n# ==== Pivot ===\ndf.groupBy(\"year\").pivot(\"quarter\").sum(\"revenue\").show()\ndf.groupBy(\"year\") .pivot(\"quarter\").agg(sum(\"revenue\").alias(\"revenue\"),sum(\"profit\").alias(\"profit\")).show()\n'''\n+----+-------+-------+-------+\n|year|quarter|revenue|profit |\n+----+-------+-------+-------+\n|2023|      1|    100|     20|\n|2023|      2|    150|     30|\n|2023|      3|    200|     40|\n|2023|      4|    120|     25|\n+----+-------+-------+-------+  \n\n+----+---+---+---+---+\n|year|  1|  2|  3|  4|\n+----+---+---+---+---+\n|2023|100|150|200|120|\n+----+---+---+---+---+\n\n+----+---------+---------+---------+---------+--------+--------+--------+--------+\n|year|1_revenue|1_profit |2_revenue|2_profit |3_revenue|3_profit|4_revenue|4_profit|\n+----+---------+---------+---------+---------+---------+--------+---------+--------+\n|2023|      100|       20|      150|       30|      200|      40|      120|      25|\n|2024|      180|       35|       90|       15|      210|      45|      130|      30|\n+----+---------+---------+---------+---------+---------+--------+---------+--------+\n'''\n\ndf.withColumn(\"item\", explode(\"items_array\")).show() # Expand arrays/maps\n\n# === Chain ===\n\n(df.filter(df.age &gt; 25)\n .groupBy(\"department\")\n .agg(avg(\"salary\").alias(\"avg_salary\"))\n .orderBy(\"avg_salary\", ascending=False)\n .show())\n</code></pre></li> </ul>"},{"location":"100_pyspark_01_start/#3-action-groupby-agg-join-order","title":"3 action: groupBy, agg, join, order","text":"<ul> <li>agg === having in sql <pre><code>df.groupBy(\"department\").count().show()\n\ndf.groupBy(\"department\").agg(\n  avg(\"salary\").alias(\"avg_salary\"),\n  max(\"age\").alias(\"max_age\")\n  ).show()\n\nemployees.join(departments,\n               employees.dept_id == departments.id,\n               \"inner\").show()\n\ndf.orderBy(\"age\").show()                  # ascending\n\ndf.orderBy(df.age.desc()).show()          # descending\n\ndf.orderBy([\"age\", \"name\"], ascending=[0, 1]).show()  # mixed\n</code></pre></li> </ul>"},{"location":"100_pyspark_01_start/#5-more","title":"5 more","text":""},{"location":"100_pyspark_01_start/#10-performance-optimize-a-slow-pyspark-job","title":"10 performance : optimize a slow PySpark job","text":""},{"location":"100_pyspark_01_start/#cache","title":"cache","text":"<pre><code># CACHE :: Data reused multiple times + Small enough to fit in cluster memory\nfrom pyspark.storagelevel import StorageLevel\ndf.persist(StorageLevel.MEMORY_AND_DISK)\n\ndf.cache()  # or df.persist(StorageLevel.MEMORY_ONLY)\ndf.unpersist() # # Don't forget to unpersist!\n</code></pre>"},{"location":"100_pyspark_01_start/#dfdropduplicates","title":"df.dropDuplicates()","text":""},{"location":"100_pyspark_01_start/#partitioning","title":"Partitioning","text":"<ul> <li>repartition(200, \"department_id\") : shuffles data to increase/decrease partitions.</li> <li>coalesce(50) : reduces partitions without a full shuffle.</li> </ul>"},{"location":"100_pyspark_01_start/#avoid-shuffles","title":"avoid shuffles.","text":"<pre><code># Bad - causes shuffle\ndf.orderBy(\"timestamp\")\n# Better - use partitioning\ndf.repartition(\"date_column\").sortWithinPartitions(\"timestamp\")\n\n# Worst - multiple shuffles\ndf.groupBy(\"a\").agg(...).groupBy(\"b\").agg(...)\n# Better - single aggregation\ndf.groupBy(\"a\", \"b\").agg(...)\n</code></pre>"},{"location":"100_pyspark_01_start/#broadcast-variable-and-join","title":"broadcast variable and join","text":"<ul> <li>a read-only variable cached on each worker to avoid shipping it multiple times </li> <li>used for small lookup table. <pre><code>from pyspark import SparkContext\nsc = SparkContext()\n# Small lookup dictionary\ncountry_codes = {    \"US\": \"United States\",    \"IN\": \"India\",    \"UK\": \"United Kingdom\"}\n\nbroadcast_codes = sc.broadcast(country_codes) # Create broadcast variable\n\n# Use in RDD operations\nrdd = sc.parallelize([\"US\", \"IN\", \"UK\", \"IN\", \"US\"])\nfull_names = rdd.map(lambda x: broadcast_codes.value[x])\nfull_names.collect()\n# Returns: ['United States', 'India', 'United Kingdom', 'India', 'United States']\n</code></pre></li> <li>Broadcast join <pre><code>from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import broadcast\n\nspark = SparkSession.builder.getOrCreate()\n\n# Large fact table\ntransactions = spark.createDataFrame([\n  (1, \"US\", 100), (2, \"IN\", 200), (3, \"UK\", 300),\n  (4, \"US\", 150), (5, \"IN\", 250)], [\"id\", \"country\", \"amount\"])\n\n# Small dimension table\ncountries = spark.createDataFrame([\n  (\"US\", \"United States\"), (\"IN\", \"India\"),\n  (\"UK\", \"United Kingdom\")], [\"code\", \"name\"])\n\n# Standard join (shuffle join)\ntransactions.join(countries, transactions.country == countries.code).show()\n\n# Broadcast join (more efficient)\ntransactions.join(broadcast(countries), transactions.country == countries.code).show()\n</code></pre></li> </ul>"},{"location":"100_pyspark_01_start/#tuning-executor-memory","title":"tuning executor memory","text":"<pre><code>spark-submit \\\n  --executor-memory 8G \\\n  --executor-cores 4 \\\n  --num-executors 10 \\\n  --conf spark.memory.fraction=0.8 \\\n  --conf spark.memory.storageFraction=0.3 \\\n  your_app.py\n</code></pre>"},{"location":"100_pyspark_01_start/#file-format-optimization","title":"File Format Optimization","text":"<ul> <li>df.write.parquet(\"output.parquet\")</li> <li>df.write.format(\"avro\").save(\"output.avro\")</li> <li>df.repartition(100).write.parquet(\"output\")  # 100 files -  Control file size</li> </ul>"},{"location":"100_pyspark_01_start/#99-scenarios","title":"99 scenarios","text":"<ul> <li>handle missing values</li> <li>df.na.fill() or df.na.drop()</li> <li>Spark SQL</li> <li>df.createOrReplaceTempView(\"table\")</li> <li>spark.sql(\"SELECT * FROM table\")</li> </ul>"},{"location":"100_pyspark_02/","title":"100_pyspark_02","text":""},{"location":"100_pyspark_02/#1-converting-rdd-to-dataframe","title":"1. Converting RDD to DataFrame","text":"<pre><code>from pyspark.sql.types import StructType, StructField, StringType, IntegerType\nfrom pyspark.sql import SparkSession\n\nsc = SparkSession.builder.appName(\"MyApp\").getOrCreate()\ndata = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 35)]\nrdd = sc.parallelize(data)\n\nschema = StructType([\n    StructField(\"name\", StringType(), True),\n    StructField(\"age\", IntegerType(), True)\n])\n\ndf = sc.createDataFrame(rdd, schema)\ndf.show() # print\ndf.explain() # prints the execution plan\n</code></pre>"}]}